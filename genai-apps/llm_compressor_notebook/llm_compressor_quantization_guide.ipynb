{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a0479f",
   "metadata": {},
   "source": [
    "\n",
    "# Guide: Quantizing LLMs with LLM Compressor for vLLM\n",
    "\n",
    "This notebook provides a step-by-step guide for quantizing a Hugging Face language model using Neural Magic's `llm-compressor`. Quantization is a crucial technique for reducing a model's memory footprint and accelerating inference speed by converting its weights from high-precision floating-point numbers (like FP32) to lower-precision integers (like INT8 or INT4).\n",
    "\n",
    "This process is essential for deploying large models efficiently on resource-constrained hardware. We will walk through the entire workflow:\n",
    "\n",
    "1.  **Setup**: Install necessary libraries.\n",
    "2.  **Configuration**: Define the model and quantization parameters.\n",
    "3.  **Download**: Fetch the base model from the Hugging Face Hub.\n",
    "4.  **Quantize**: Apply a one-shot quantization recipe using `llm-compressor`.\n",
    "5.  **Evaluate**: (Optional) Measure the performance of the quantized model.\n",
    "6.  **Upload**: (Optional) Push the quantized model to an S3-compatible object store.\n",
    "\n",
    "This guide is designed for AI Platform Engineers and Consultants who are building and optimizing AI services. The final quantized model will be compatible with high-performance inference engines like vLLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db82456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Setup: Install required packages\n",
    "# Note: This process requires a GPU for both quantization and evaluation.\n",
    "%pip install llmcompressor accelerate vllm datasets transformers torch lm_eval==0.4.3 huggingface-hub boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a697fa",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Configuration\n",
    "\n",
    "Here, we'll define all the key parameters for our quantization process. You can easily change these values to quantize a different model or adjust the quantization settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5450eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# --- Model Configuration ---\n",
    "# The Hugging Face model ID to download and quantize.\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-2b-instruct\"\n",
    "\n",
    "# --- Quantization Configuration ---\n",
    "# Choose \"int8\" or \"int4\".\n",
    "# INT8 offers a good balance of performance and accuracy.\n",
    "# INT4 provides maximum compression but may have a higher accuracy loss.\n",
    "QUANTIZATION_TYPE = \"int8\"\n",
    "\n",
    "# --- Path Configuration ---\n",
    "# Directory to store the original, full-precision model.\n",
    "BASE_MODEL_PATH = \"base_model\"\n",
    "# Directory to save the final quantized model.\n",
    "OPTIMIZED_MODEL_PATH = f\"optimized_model_{QUANTIZATION_TYPE}\"\n",
    "\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  - Model ID: {MODEL_ID}\")\n",
    "print(f\"  - Quantization Type: {QUANTIZATION_TYPE}\")\n",
    "print(f\"  - Base Model Path: ./{BASE_MODEL_PATH}\")\n",
    "print(f\"  - Optimized Model Path: ./{OPTIMIZED_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cd0e8",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Download the Base Model\n",
    "\n",
    "First, we download the pre-trained model weights and tokenizer from the Hugging Face Hub using the `snapshot_download` function. This saves the complete model repository to our local `BASE_MODEL_PATH`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "print(f\"Downloading model '{MODEL_ID}' from Hugging Face Hub...\")\n",
    "snapshot_download(repo_id=MODEL_ID, local_dir=BASE_MODEL_PATH)\n",
    "print(f\"Model downloaded successfully to ./{BASE_MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be48a4",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Quantize the Model\n",
    "\n",
    "This is the core of the notebook. We will perform one-shot quantization, which compresses the model without requiring a full retraining cycle. This process involves several sub-steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63046d",
   "metadata": {},
   "source": [
    "\n",
    "### 4.1. Load Model and Tokenizer\n",
    "\n",
    "We load the downloaded model and its tokenizer using the `transformers` library. We specify `device_map=\"auto\"` to ensure the model is loaded onto the available GPU(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH, device_map=\"auto\", torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "print(\"Model and tokenizer loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0d2de",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2. Prepare Calibration Data\n",
    "\n",
    "Quantization algorithms need a small, representative sample of data to \"calibrate\" the model's weight distributions before converting them to integers. This helps minimize accuracy loss. We'll use a standard calibration dataset from Neural Magic and preprocess it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Parameters for data calibration\n",
    "NUM_CALIBRATION_SAMPLES = 256\n",
    "DATASET_ID = \"neuralmagic/LLM_compression_calibration\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "print(f\"Loading and preparing calibration dataset: {DATASET_ID}\")\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "ds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\n",
    "ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n",
    "\n",
    "def preprocess(example):\n",
    "    return {\"text\": example[\"text\"]}\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)\n",
    "\n",
    "print(\"Calibration data is ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd99c83e",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3. Define the Quantization Recipe\n",
    "\n",
    "A \"recipe\" in `llm-compressor` is a set of instructions that defines how to modify the model. We'll use the GPTQ (Generative Pre-trained Transformer Quantization) algorithm. For INT8, we'll also add SmoothQuant as a pre-processing step to make quantization more effective.\n",
    "\n",
    "The parameters below (`DAMPENING_FRAC`, `GROUP_SIZE`, etc.) are hyperparameters for the quantization algorithm that can be tuned for optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbaba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "\n",
    "# Hyperparameters for quantization\n",
    "DAMPENING_FRAC = 0.1\n",
    "OBSERVER = \"mse\"\n",
    "GROUP_SIZE = 128\n",
    "ignore = [\"lm_head\"]\n",
    "\n",
    "print(f\"Creating quantization recipe for type: {QUANTIZATION_TYPE}\")\n",
    "\n",
    "if QUANTIZATION_TYPE == \"int8\":\n",
    "    # For INT8, we use SmoothQuant + GPTQ.\n",
    "    # SmoothQuant shifts the quantization difficulty from activations to weights.\n",
    "    mappings=[\n",
    "        [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*input_layernorm\"],\n",
    "        [[\"re:.*gate_proj\", \"re:.*up_proj\"], \"re:.*post_attention_layernorm\"],\n",
    "        [[\"re:.*down_proj\"], \"re:.*up_proj\"]\n",
    "    ]\n",
    "    recipe = [\n",
    "        SmoothQuantModifier(smoothing_strength=0.7, ignore=ignore, mappings=mappings),\n",
    "        GPTQModifier(\n",
    "            targets=[\"Linear\"],\n",
    "            ignore=ignore,\n",
    "            scheme=\"W8A8\",  # 8-bit weights, 8-bit activations\n",
    "            dampening_frac=DAMPENING_FRAC,\n",
    "            observer=OBSERVER,\n",
    "        )\n",
    "    ]\n",
    "elif QUANTIZATION_TYPE == \"int4\":\n",
    "    # For INT4, we use GPTQ directly.\n",
    "    recipe = [\n",
    "        GPTQModifier(\n",
    "            targets=[\"Linear\"],\n",
    "            ignore=ignore,\n",
    "            scheme=\"w4a16\",  # 4-bit weights, 16-bit activations\n",
    "            dampening_frac=DAMPENING_FRAC,\n",
    "            observer=OBSERVER,\n",
    "            group_size=GROUP_SIZE\n",
    "        )\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError(f\"Quantization type {QUANTIZATION_TYPE} not supported\")\n",
    "\n",
    "print(\"Recipe created successfully.\")\n",
    "print(recipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2919daa2",
   "metadata": {},
   "source": [
    "\n",
    "### 4.4. Apply the Recipe with `oneshot`\n",
    "\n",
    "The `oneshot` function from `llm-compressor` applies our recipe to the model. It uses the calibration data we prepared to execute the quantization process without any backpropagation or gradient updates, making it very fast.\n",
    "\n",
    "The `max_seq_length` should be set based on your model's context window. We use 8196 for the Granite model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llmcompressor.transformers import oneshot\n",
    "\n",
    "print(\"Applying one-shot quantization... This may take several minutes.\")\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    max_seq_length=8196,\n",
    ")\n",
    "\n",
    "print(\"One-shot quantization complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8af072",
   "metadata": {},
   "source": [
    "\n",
    "### 4.5. Save the Quantized Model\n",
    "\n",
    "Finally, we save the modified model to disk. It's crucial to set `save_compressed=True` to ensure the quantization changes are correctly stored in a format that engines like vLLM can load. We also save the tokenizer for completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b834ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Saving quantized model to ./{OPTIMIZED_MODEL_PATH}...\")\n",
    "\n",
    "# Save to disk in compressed format\n",
    "model.save_pretrained(OPTIMIZED_MODEL_PATH, save_compressed=True)\n",
    "tokenizer.save_pretrained(OPTIMIZED_MODEL_PATH)\n",
    "\n",
    "print(\"Quantized model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caee93f",
   "metadata": {},
   "source": [
    "\n",
    "## 5. (Optional) Evaluate the Model\n",
    "\n",
    "After quantization, it's important to evaluate the model's performance on a standard benchmark to understand any potential accuracy degradation. We'll use the `lm-eval-harness` to run the `gsm8k` benchmark, which tests grade-school math reasoning.\n",
    "\n",
    "**Note:** This step requires a GPU and will be executed via a subprocess. The results will be printed below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "# Check for GPU\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Evaluation skipped: No GPU detected.\")\n",
    "else:\n",
    "    print(\"Starting model evaluation with lm-eval-harness...\")\n",
    "    command = [\n",
    "        \"lm_eval\",\n",
    "        \"--model\", \"vllm\",\n",
    "        \"--model_args\", f\"pretrained={OPTIMIZED_MODEL_PATH},add_bos_token=true,dtype=auto\",\n",
    "        \"--tasks\", \"gsm8k\",\n",
    "        \"--num_fewshot\", \"5\",\n",
    "        \"--limit\", \"250\",\n",
    "        \"--batch_size\", \"auto\",\n",
    "        \"--trust_remote_code\"\n",
    "    ]\n",
    "\n",
    "    # Execute the command\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    # Print the output\n",
    "    if result.returncode == 0:\n",
    "        print(\"Model evaluated successfully:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"Error evaluating the model:\")\n",
    "        print(result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4e12a",
   "metadata": {},
   "source": [
    "\n",
    "## 6. (Optional) Upload Model to S3\n",
    "\n",
    "For production workflows, you'll often need to store your model artifacts in a central object store. This section shows how to upload the entire quantized model directory to an S3-compatible bucket using `boto3`.\n",
    "\n",
    "**Action Required**: You must configure your S3 credentials below for this step to work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48909c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "\n",
    "# --- S3 Configuration ---\n",
    "# IMPORTANT: Replace these with your actual S3 details.\n",
    "# For security, it's best to load these from environment variables or a secrets manager.\n",
    "S3_ENDPOINT_URL = \"https://s3.example.com\"  # e.g., 'https://s3.us-east-1.amazonaws.com'\n",
    "S3_ACCESS_KEY = \"YOUR_ACCESS_KEY\"\n",
    "S3_SECRET_KEY = \"YOUR_SECRET_KEY\"\n",
    "S3_BUCKET_NAME = \"your-models-bucket\"\n",
    "S3_PATH_IN_BUCKET = f\"quantized-models/{MODEL_ID.replace('/', '_')}-{QUANTIZATION_TYPE}\"\n",
    "\n",
    "# --- Upload Logic ---\n",
    "def upload_to_s3(local_path, s3_bucket, s3_prefix):\n",
    "    if S3_ACCESS_KEY == \"YOUR_ACCESS_KEY\":\n",
    "        print(\"Upload skipped: S3 credentials not configured.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Starting upload to bucket '{s3_bucket}' at '{S3_ENDPOINT_URL}'...\")\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=S3_ENDPOINT_URL,\n",
    "        aws_access_key_id=S3_ACCESS_KEY,\n",
    "        aws_secret_access_key=S3_SECRET_KEY,\n",
    "        verify=False # Set to True if you have valid SSL certs\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for root, dirs, files in os.walk(local_path):\n",
    "            for file in files:\n",
    "                local_file_path = os.path.join(root, file)\n",
    "                # Create a relative path for S3\n",
    "                s3_file_path = os.path.join(s3_prefix, os.path.relpath(local_file_path, local_path))\n",
    "                s3_client.upload_file(local_file_path, s3_bucket, s3_file_path)\n",
    "                print(f\"  - Uploaded {s3_file_path}\")\n",
    "        print(\"Finished uploading results.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during upload: {e}\")\n",
    "\n",
    "# Execute the upload\n",
    "upload_to_s3(\n",
    "    local_path=OPTIMIZED_MODEL_PATH,\n",
    "    s3_bucket=S3_BUCKET_NAME,\n",
    "    s3_prefix=S3_PATH_IN_BUCKET\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c9ad6",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "You have successfully quantized a large language model using `llm-compressor`. The resulting model is smaller, faster for inference, and ready to be served with vLLM.\n",
    "\n",
    "From here, you can:\n",
    "- Integrate the quantized model path into your vLLM deployment configurations.\n",
    "- Experiment with `int4` quantization for even greater compression.\n",
    "- Fine-tune the quantization hyperparameters (`DAMPENING_FRAC`, `GROUP_SIZE`, etc.) to optimize the trade-off between performance and accuracy for your specific use case.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
