{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing: Summarize Support Tickets with an LLM\n",
    "\n",
    "This notebook demonstrates a practical use case for a deployed Large Language Model: batch processing. We will simulate a scenario where we have a folder of support tickets (in JSON format) and use our model to automatically:\n",
    "\n",
    "1.  Read each ticket.\n",
    "2.  Generate a concise, one-sentence summary of the issue.\n",
    "3.  Suggest potential troubleshooting steps.\n",
    "4.  Save this analysis into a new summary file.\n",
    "\n",
    "This workflow can significantly speed up ticket triage and analysis for a support team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "First, we'll ensure the necessary Python libraries are installed. `langchain-openai` helps us communicate with the model, and `httpx` is the HTTP client that handles the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langchain-openai httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration\n",
    "\n",
    "Next, we'll import the libraries and configure the connection to our deployed model. \n",
    "\n",
    "**Action Required:**\n",
    "1.  **`BASE_URL`**: Replace the placeholder with the **Inference endpoint** URL from your model's details page.\n",
    "2.  **`API_KEY`**: Replace the placeholder with the **Authentication Token** from the 'Authentication' section of the model's details page.\n",
    "3.  **`MODEL_NAME`**: Ensure this matches the name of your model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import httpx\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# --- \u2757ACTION REQUIRED: REPLACE VALUES BELOW \u2757---\n",
    "\n",
    "# Model and Connection Details\n",
    "MODEL_NAME = \"granite\" # The name of your model deployment\n",
    "BASE_URL = \"https://your-model-inference-url.apps.cluster.com/v1\" # Replace with your Inference Endpoint\n",
    "API_KEY = \"sha256~your-long-authentication-token\" # Replace with your Authentication Token\n",
    "\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Directory and File Handling Configuration\n",
    "INPUT_FOLDER = \"support_tickets\"\n",
    "# This is the key in the JSON file that contains the main ticket text.\n",
    "JSON_TICKET_KEY = \"issue_description\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Sample Data\n",
    "\n",
    "For this notebook to be self-contained, we will programmatically create the `support_tickets` directory and populate it with a few sample JSON files. In a real-world scenario, this folder would already exist and be populated with tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input folder if it doesn't exist\n",
    "os.makedirs(INPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Define sample ticket data\n",
    "ticket1_data = {\n",
    "    \"ticket_id\": \"TICK-001\",\n",
    "    \"customer_name\": \"Alice\",\n",
    "    \"issue_description\": \"I'm trying to log into the main dashboard, but after I enter my password, the page just spins and never loads. I've tried clearing my browser cache and using a different browser (Chrome and Firefox), but the issue persists. My colleague can log in just fine.\"\n",
    "}\n",
    "\n",
    "ticket2_data = {\n",
    "    \"ticket_id\": \"TICK-002\",\n",
    "    \"customer_name\": \"Bob\",\n",
    "    \"issue_description\": \"The monthly sales report is failing to generate. When I click the 'Export to PDF' button, I receive a generic 'Error 500 - Internal Server Error' message. This was working yesterday. I need this report for a meeting this afternoon.\"\n",
    "}\n",
    "\n",
    "# Write the sample tickets to JSON files\n",
    "with open(os.path.join(INPUT_FOLDER, \"ticket_001.json\"), 'w') as f:\n",
    "    json.dump(ticket1_data, f, indent=2)\n",
    "\n",
    "with open(os.path.join(INPUT_FOLDER, \"ticket_002.json\"), 'w') as f:\n",
    "    json.dump(ticket2_data, f, indent=2)\n",
    "\n",
    "print(f\"Successfully created sample files in the '{INPUT_FOLDER}' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize the LLM Client\n",
    "\n",
    "Now, we set up the `ChatOpenAI` client with our connection details. We'll set a low `temperature` to encourage the model to provide more factual, less creative responses, which is ideal for a support context. \n",
    "\n",
    "**Note:** We are using `verify=False` to disable SSL verification, which is often necessary in development or sandboxed OpenShift environments. This is not recommended for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=MODEL_NAME,\n",
    "    api_key=API_KEY,\n",
    "    base_url=BASE_URL,\n",
    "    http_client=httpx.Client(verify=False),\n",
    "    temperature=0.1,  # Lower temperature for more factual responses\n",
    ")\n",
    "\n",
    "print(\"LLM client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the Core Processing Function\n",
    "\n",
    "This function takes the text from a single support ticket, wraps it in a carefully crafted prompt, and sends it to the LLM. The prompt instructs the model to act as a senior support engineer, providing both a summary and potential solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_summarize_ticket(ticket_content):\n",
    "    \"\"\"\n",
    "    Sends ticket content to the LLM for summarization and solution generation.\n",
    "    \"\"\"\n",
    "    # Define a clear, instructional prompt for the model\n",
    "    prompt = f\"\"\"\n",
    "    You are a senior support engineer. Please perform the following tasks for the support ticket below:\n",
    "    1. Provide a concise one-sentence summary of the user's issue.\n",
    "    2. Based on the issue, list 3 to 4 potential troubleshooting steps or solutions in a numbered list.\n",
    "\n",
    "    Here is the support ticket:\n",
    "    ---\n",
    "    {ticket_content}\n",
    "    ---\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful senior support engineer.\"),\n",
    "        HumanMessage(content=prompt),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        ai_msg = llm.invoke(messages)\n",
    "        return ai_msg.content\n",
    "    except Exception as e:\n",
    "        return f\"Error communicating with the model: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Batch Process\n",
    "\n",
    "This is the main execution logic. The code will:\n",
    "1. Find all `.json` files in our `support_tickets` directory.\n",
    "2. Loop through each file.\n",
    "3. Read the content and extract the ticket description.\n",
    "4. Call our `analyze_and_summarize_ticket` function.\n",
    "5. Save the model's response to a new `_summary.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all JSON files in the directory\n",
    "json_files = [f for f in os.listdir(INPUT_FOLDER) if f.endswith('.json')]\n",
    "\n",
    "if not json_files:\n",
    "    print(f\"No JSON files found in the '{INPUT_FOLDER}' directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(json_files)} support tickets to process...\")\n",
    "    for file_name in json_files:\n",
    "        json_path = os.path.join(INPUT_FOLDER, file_name)\n",
    "        print(f\"\\nProcessing: {file_name}\")\n",
    "\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Extract the ticket text using the specified key\n",
    "            ticket_text = data.get(JSON_TICKET_KEY)\n",
    "\n",
    "            if not ticket_text:\n",
    "                print(f\"  - Skipping: Could not find key '{JSON_TICKET_KEY}' in the file.\")\n",
    "                continue\n",
    "\n",
    "            # Get the summary and solutions from the model\n",
    "            summary_and_solutions = analyze_and_summarize_ticket(ticket_text)\n",
    "\n",
    "            # Define the output filename\n",
    "            base_name = os.path.splitext(file_name)[0]\n",
    "            output_filename = f\"{base_name}_summary.txt\"\n",
    "            output_path = os.path.join(INPUT_FOLDER, output_filename)\n",
    "\n",
    "            # Save the result to a text file\n",
    "            with open(output_path, 'w') as out_f:\n",
    "                out_f.write(summary_and_solutions)\n",
    "\n",
    "            print(f\"  - Successfully saved summary to: {output_filename}\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"  - Skipping: Invalid JSON format in {file_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - An unexpected error occurred: {e}\")\n",
    "\n",
    "    print(\"\\nProcessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify the Output\n",
    "\n",
    "Finally, let's check our work. The code below will list all the files in the `support_tickets` directory (you should see the new `_summary.txt` files) and then print the content of the first summary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Contents of '{INPUT_FOLDER}' directory ---\")\n",
    "for file in os.listdir(INPUT_FOLDER):\n",
    "    print(file)\n",
    "print(\"----------------------------------------\\n\")\n",
    "\n",
    "# Print the content of the first summary file\n",
    "summary_file_path = os.path.join(INPUT_FOLDER, \"ticket_001_summary.txt\")\n",
    "if os.path.exists(summary_file_path):\n",
    "    print(f\"--- Content of {os.path.basename(summary_file_path)} ---\")\n",
    "    with open(summary_file_path, 'r') as f:\n",
    "        print(f.read())\n",
    "    print(\"----------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}