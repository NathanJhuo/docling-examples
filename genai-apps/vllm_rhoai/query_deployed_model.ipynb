{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query a Deployed Model on Red Hat OpenShift AI\n",
    "\n",
    "This notebook demonstrates how to connect to and query a large language model that has already been deployed using the Single-Model Serving feature in a Red Hat OpenShift AI data science project. \n",
    "\n",
    "We will use the `langchain-openai` library to create a client that can communicate with any OpenAI-compatible API endpoint, which is what the vLLM runtime provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "First, we need to install the necessary Python libraries. `langchain-openai` provides the tools to interact with the model endpoint, and `httpx` is the underlying HTTP client used to make the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-openai httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Model Connection\n",
    "\n",
    "Next, we need to configure the connection to our deployed Granite model. You must replace the placeholder values below with the specific details from your model's deployment page in OpenShift AI.\n",
    "\n",
    "**Action Required:**\n",
    "1.  **`BASE_URL`**: Replace the placeholder with the **Inference endpoint** URL from your model's details page.\n",
    "2.  **`API_KEY`**: Replace the placeholder with the **Authentication Token** from the 'Authentication' section of the model's details page.\n",
    "3.  **`MODEL_NAME`**: This should be the name you gave your deployment (e.g., `granite`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# --- ❗ACTION REQUIRED: REPLACE THESE VALUES ❗---\n",
    "MODEL_NAME = \"granite\" # Or the specific name of your model deployment\n",
    "BASE_URL = \"https://your-model-inference-url.apps.cluster.com/v1\" # Replace with your model's Inference endpoint\n",
    "API_KEY = \"sha256~your-long-authentication-token\" # Replace with your Authentication Token\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Optional: If your cluster uses self-signed certificates, you may need to disable SSL verification.\n",
    "# Note: This is not recommended for production environments.\n",
    "# http_client = httpx.Client(verify=False)\n",
    "\n",
    "try:\n",
    "    # Initialize the ChatOpenAI client\n",
    "    llm = ChatOpenAI(\n",
    "        model=MODEL_NAME,\n",
    "        api_key=API_KEY,\n",
    "        base_url=BASE_URL,\n",
    "        # Uncomment the line below if you need to disable SSL verification\n",
    "        # http_client=http_client,\n",
    "    )\n",
    "\n",
    "    print(\"Configuration successful. Client is ready.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during client initialization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Send a Request to the Model\n",
    "\n",
    "Now that the client is configured, we can send a request to the model. We construct a list of messages, including a `SystemMessage` to set the model's behavior and a `HumanMessage` with our question. Then, we use the `llm.invoke()` method to get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Prepare the messages for the model\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant who provides concise answers.\"),\n",
    "        HumanMessage(content=\"What is OpenShift AI?\"),\n",
    "    ]\n",
    "\n",
    "    # Invoke the model and get the response\n",
    "    print(\"Sending request to the Granite model...\")\n",
    "    ai_msg = llm.invoke(messages)\n",
    "\n",
    "    # Print the content of the response\n",
    "    print(\"\\nResponse from Granite Model:\")\n",
    "    print(ai_msg.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment!\n",
    "\n",
    "Now it's your turn. Go back to the previous code cell, change the content of the `HumanMessage` to your own question, and run the cell again to see how the model responds. \n",
    "\n",
    "**Example questions:**\n",
    "* `\"Write a python function that calculates the factorial of a number.\"`\n",
    "* `\"What are the key benefits of using a GPU for deep learning?\"`\n",
    "* `\"Explain the difference between object storage and file storage.\"`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
